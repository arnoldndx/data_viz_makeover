---
title: "Assignment"
description: |
  My approach and solution to Mini Challenge 1 of the VAST Challenge 2021.
author:
  - name: Arnold Ng
    url: {}
date: 07-16-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,
                      tidy.opts=list(width.cutoff=40),
                      tidy=TRUE,
                      message = FALSE, 
                      warnings = FALSE)
```

```{css, echo=FALSE}
#format table captions
caption {
  font-size: 20px;
  font-weight: bold;
}
```

```{r echo=FALSE}
#load relevant libraries
packages = c('DT','tidyverse','tidygraph','igraph',
             'tidytext','readtext','lubridate','formatR',
             'textstem', 'tm', 'slam', 'ggalluvial',
             'ggrepel','wordcloud','RColorBrewer','graphics',
             'gbutils','ggraph',
             'clock','visNetwork','kableExtra')

for (p in packages){
  if(!require(p,character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

#set working directory to reference data files
setwd("~/0. SMU MITB/9. ISSS608 Visual Analytics & Applications/arnoldndx/data_viz_makeover/_posts/2021-07-16-assignment")
```
# 1. Task

The task is to use visual analytics to examine text data comprising:

* news articles; 
* GAStech employee email headers;
* GAStech employee records; and
* GAStech employee resumes

in order to answer the following questions.

**QUESTION 1:** Characterize the news data sources provided. Which are primary sources and which are derivative sources? What are the relationships between the primary and derivative sources? Please limit your answer to 8 images and 300 words.

**QUESTION 2:** Characterize any biases you identify in these news sources, with respect to their representation of specific people, places, and events. Give examples. Please limit your answer to 6 images and 500 words.

**QUESTION 3:** Given the data sources provided, use visual analytics to identify potential official and unofficial relationships among GASTech, POK, the APA, and Government. Include both personal relationships and shared goals and objectives. Provide evidence for these relationships. Please limit your answer to 6 images and 400 words.

# 2. Literature Review and Methods

## 2.1. Literature Review

Past attempts at the VAST Challenge 2014, which uses the same dataset, was reviewed before attempting this assignment. The attempts were retrived from the VAST Challenge [Visual Analytics Benchmark Repository](https://www.cs.umd.edu/hcil/varepository/VAST%20Challenge%202014/challenges/MC1%20-%20Disappearance%20at%20GASTech/) hosted by the University of Maryland.

Firstly, we noticed that most groups used alot of manual inspection of the text data, rather than more sophisticated text analysis. This is also why most groups spend upwards of 150 hours analysing the data to draw conclusions. As we are doing an individul and not a group assignment, our resources will be much more limited. For our analysis, we will attempt to use slightly more sophisticated analysis at of the text data at an aggregated level so as to draw some conclusions regarding the data more efficiently.

TianJin University's [submission](https://www.cs.umd.edu/hcil/varepository/VAST%20Challenge%202014/challenges/MC1%20-%20Disappearance%20at%20GASTech/entries/Tianjin%20University%20-%20Cai/) did present a more sophisticated cluster analysis of the news articles to identify media bias, but they did not provide details on how they approached the clustering. We will instead develop a new approach to our media bias analysis based on customised sentiment analysis.

To analyse the relationships between entities for part 3, most used variants of network graphs and Sankey diagrams. We will adopt a similar approach in analysing the relationships.


## 2.2. R Packages used

The following R packages are used in this assignment:

R Package | Usage
----------|------------
tidyverse,  formatR,  gbutils | Utilities for data wrangling
lubridate,  clock | Utilities to manage dates
DT, kableExtra | To produce data tables for viewing data
tidytext,  readtext,  textstem,  tm,  slam | Utilities for text data wrangling and matrix multiplications
graphics,  ggrepel,  RcolorBrewer | Utilities for graph plotting
ggalluvial | Plotting Sankey/Alluvial plots
wordcloud | Plotting wordclouds
visNetwork | Plotting network graphs


# 3. Data Preparation

Firstly, we will need to clean up and process the data into data tables to facilitate analysis. We will use a combination of R tools and manual inspection/editing to clean up the data.

There are three main datasets that will need to be processed and tidied up. The first set comprises the news articles. They will have to be collected into a data table for analysis, with key article details such as title, author, publish date, etc. extracted to facilitate analysis. The second set of data comprises the employee relationships to each other, as well as entities such as countries, organisations, persons outside GAStech, etc. The third and final set of data comprises email headers for two weeks worth of emails, along with labels of the type of emails. 

## 3.1. News Articles

There is noise in the news article data, which includes incosistent line breaks, inconsistent formatting of article details such as dates, author names, etc., and inconsistent details provided per article. This has to be taken into account in processing the article data.

### 3.1.1. Combine articles into single table for checking

We first begin by combining the article data into a single table to facilitate checks. We get the easy part out of the way and clean up the line breaks, by replacing all double line breaks with single line breaks with the code below.

```{r eval=FALSE}
articles <- readtext("MC 1/News Articles/*")

for (row in 1:nrow(articles)){
  #clean up the text of breaks
  while (str_detect(articles[row,'text'],'\n ')){
    articles[row,'text'] <- str_replace_all(articles[row,'text'],"\n ","\n")
  }
  while (str_detect(articles[row,'text'],'\n\n')){
    articles[row,'text'] <- str_replace_all(articles[row,'text'],"\n\n","\n")
  }
}
```

We then export the data for manual cleaning (i.e. edit the text directly in the csv file).

```{r eval=FALSE}
#export data for manual cleaning
write.csv(articles,"article_consol.csv")
```

### 3.1.2. Load cleaned data and convert to article data into analysable dataframe

We now begin the task of converting the data into an analysable format. 

First, we perform a first pass manual check of all the article text and manually clean up the data into a consistent format.Then we load in the cleaned data.

```{r}
#load in cleaned data
article_data <- read.csv("article_consol_edit.csv")
```

Next, we convert the article data into an analysable dataframe to with the relevant article information separated out. We also ensure that the published dates are properly formatted for analysis.

```{r, message = FALSE, warnings = FALSE}
#create dataframe to store article data
article_df <- data_frame(doc_id = rep('',nrow(article_data)),
                         source = rep('',nrow(article_data)),
                         title = rep('',nrow(article_data)),
                         author = rep('',nrow(article_data)),
                         location = rep('',nrow(article_data)),
                         published = rep('',nrow(article_data)),
                         text = rep('',nrow(article_data)))

#define the article details to be captured
headers <- c('source',
             'title',
             'author',
             'location',
             'published')

for (row in 1:nrow(article_data)){
  article_text <- str_split(article_data$text[row],'\n')
  article_df$doc_id[row] <- article_data$doc_id[row]
  for (i in 1:length(article_text[[1]])){
    detail <- str_trim(str_split_fixed(article_text[[1]][i],":",2))
    # check if any of the string headers corresponds to article details, otherwise treat as body text
    if (tolower(detail[1]) %in% headers){
      #some of the article text has "LOCATION:", especially police blotter transcripts, this is to make sure only the first instance of location is used.
      if (article_df[row,tolower(detail[1])] == ''){
        #for multiple authors store authors as list so that they can be referenced separately
        if (tolower(detail[1]) == 'author'){
          article_df[row,tolower(detail[1])] <- list(str_trim(str_split(detail[2],',')))
        } else if (tolower(detail[1]) == 'published'){
          date <- str_replace_all(str_replace_all(detail[2],'  ',' '),',','')
          article_df[row,'published'] <- date
        }
        article_df[row,tolower(detail[1])] <- str_trim(detail[2])
      } else {
        #concatenate body text
        article_df[row,'text'] <- paste(article_df[row,'text'],article_text[[1]][i], sep='\n')
      }
    } else {
      #concatenate body text
      article_df[row,'text'] <- paste(article_df[row,'text'],article_text[[1]][i], sep='\n')
    }
  } 
}

#convert the published dates to dates
article_df$published <- as.Date(parse_date_time(article_df$published,c('dmy','mdy','ymd')))

```
And we're done with the news article data!

## 3.2. GAStech employee data

The GAStech employee data is already neatly captured in tabular form and will not need significant pre-processing. Just load it in!

```{r}
employee_data <- read.csv('MC 1/EmployeeRecords.csv')
```

## 3.3. Email headers

We will load in the email data and expand the email headers list. Every row should be a unique combination of individual sender, individual recipient, and subject. This will facilitate network analysis later.

```{r eval=FALSE}
email_header <- read_csv('MC 1/email headers.csv')

#note that row 553 has an error in the date time, impute a time
email_header$Date[553] <- '1/13/2014 9:00'

email_header_proc <- c('source',
                       'target',
                       'sent_date',
                       'sent_time',
                       'subject')

#convert email headers to a processed dataframe
for (i in 1:nrow(email_header)){
  #parse the date times
  date <- format(mdy_hm(email_header$Date[i]),'%Y-%m-%d')
  time <- format(mdy_hm(email_header$Date[i]),'%I:%M:%S %p')
  #create a new row for each recepient
  for (j in 1:length(str_split(email_header[i,2],',')[[1]])){
    source <- email_header[i,1]
    target <- str_trim(str_split(email_header[i,2],',')[[1]][j])
    sent_date <- date
    sent_time <- time
    subject <- email_header[i,4]
    new_row <- c(source, 
                 target, 
                 sent_date,
                 sent_time,
                 subject)
    email_header_proc <- rbind(email_header_proc,new_row)
  }
}

#remove top row
email_header_proc <- email_header_proc[-1,]

#reset rownames and colnames
rownames(email_header_proc) <- c(1:nrow(email_header_proc))
colnames(email_header_proc) <- c('source',
                                 'target',
                                 'sent_date',
                                 'sent_time',
                                 'subject')

```

And we're done with email headers!

# 4. Solutions (Approach & Answers)

Using the prepared data, we now answer the questions with the help of some meaningful visualisations.

## 4.1. QUESTION 1
_Characterize the news data sources provided. Which are primary sources and which are derivative sources? What are the relationships between the primary and derivative sources? Please limit your answer to 8 images and 300 words._

### 4.1.1. Solution Approach and Data Wrangling

There are two ways to detect derivative sources:

1. presence of other news source name in the text (e.g. quoting or crediting another news source)
2. same title and author as another article (e.g. reprinting or republishing a previous article from another news source)

**STEP 1:** Identify presence of other news source name

For the first approach, we will establish a listing of all news sources, and then check each article text for the presence of other news sources. We will use the detects to generate an adjacency matrix to visualise the links between articles. The relationship will be defined as follows:

1. The primary source will be the news source quoted
2. The derivative source is the article doing the quoting
3. The relationship is defined as "Quoting"

```{r}
news_sources <- c(unique(article_df$source))

#append one column for each news source to article df to establish an adjacency matrix
article_df[news_sources] <- NA

#mark TRUE whenever mentions of other news sources are detected
for (row in 1:nrow(article_df)){
  for (news_source in news_sources){
    if (article_df[row,'source'] != news_source){
      article_df[row,news_source] <- str_detect(article_df[row,'text'], news_source)
    }
    else{article_df[row,news_source] <- FALSE}
  }
}

#convert the adjacency matrix into a list of edges (primary doc_id, primary source, derivative doc_id, derivative source, edge type)
edge_list_quote <- article_df[,c(1:2,8:36)] %>%
  pivot_longer(cols = !c(doc_id,source),
               names_to = 'primary_source')

#filter out the 'False' (i.e. no links)
edge_list_quote <- edge_list_quote %>%
  filter(value)

#remove the 'value' column
edge_list_quote <- edge_list_quote[,c(1:3)]

#add the additional edge details
edge_list_quote$primary_doc_id <- '' #no doc_ids for quoted source, unfortunately
edge_list_quote$type <- 'Quoting'

#rename the columns
edge_list_quote <- edge_list_quote %>%
  rename(
    derivative_doc_id = doc_id,
    derivative_source = source
  )

#reorder the columns
edge_list_quote <- edge_list_quote[,c(4,3,1,2,5)]

#remove adjacency matrix from article_df
article_df <- article_df[,1:7]

#rename the rows in article_df to doc_id for easier referencing
rownames(article_df) <- article_df$doc_id

edge_list_quote %>%
  kbl(caption = 'List of "Quoting" relationships',
      col.names = c("Primary doc ID",
                    "Primary news source",
                    "Derivative document ID",
                    "Derivative news source",
                    "Relationship Type")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```
There are only six quoting relationships detected in Step 1. We will see if we can derive more insight from the data in Step 2.

**STEP 2:** Identify articles with same title and author

First we compile a list of unique title-author combinations and extract the earliest publication of each combination. This list will be the basis for comparison against all the articles. Any articles published after the earliest publication is necessarily a reprint of the primary source. 

```{r, layout="l-body-outset"}
#establish a list of unique article title/author combinations and record the earliest publish date and the source

article_df_grouped <-  article_df %>%
  group_by(title,author) %>%
  arrange(published, by_group = TRUE) %>%
  summarise(first_doc_id = first(doc_id), 
            published = first(published), 
            news_source = first(source),
            text = first(text),
            #the below details are for further analysis
            count = n(), #count the number of times the title-author combo been published
            all_source = list(unique(source)), #list the number of news sources that have used this article
            all_doc = list(doc_id)) #list all the documents that share this article-author combination

datatable(article_df_grouped[,c(1:5,7:9)],
          class = 'cell-border stripe',
          caption = 'TABLE 2. List of unique article title-author combinations with number of repeats and sources that have published',
          rownames = FALSE,
          colnames = c("Unique title",
                       "Unique author",
                       "First document ID",
                       "First published date",
                       "First published news source",
                       "Repeat count",
                       "All sources that have published this article",
                       "All articles with this title-author combination"))
```
One quick observation that we can make is that there are several instances where articles share titles (i.e. exact match) with one combination with an author's name indicated but another combination without an author's name. This indicates that there might be situations either where one source is plagarising another or authors with one publication are re-using their work with another publication without using their bylines.

To further investigate this we shall extract the text of all articles which share the same title for comparison. We should note that if there is only one news source that has used this title (even if there are multiple articles), then this article could be a topic header for a developing story, with periodic updates being provided. In such a case, we ignore it as it does not show a primary-derivative source relationship.

```{r, layout="l-body-outset", fig.width = 20}
#filter the article data to just those with identical titles, we will use this repeats table to extract the relevant article texts

article_df_repeats <- article_df %>%
  group_by(title) %>%
  summarise(count_articles = n(),
            num_unique_sources = length(list(unique(source))[[1]]), #count the number of news sources that have used this article title
            all_docs = list(doc_id) #list all the documents that share this article-author combination
            ) %>%
  filter(num_unique_sources > 1)

coladd <- ncol(article_df_repeats)

article_df_repeats[(coladd+1):(coladd+max(article_df_repeats$num_unique_sources))] <- ''

for (i in 1:nrow(article_df_repeats)){
  all_doc <- article_df_repeats$all_docs[i]
  for (j in 1:length(all_doc[[1]])){
    article_df_repeats[i,coladd+j] <- paste('DOC_ID: ',
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['doc_id'][[1]],
                                            '\n',
                                            #source
                                            'SOURCE: ',
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['source'][[1]],
                                            '\n',
                                            #author
                                            'AUTHOR: ',
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['author'][[1]],
                                            '\n',
                                            #published date
                                            'PUBLISHED: ',
                                            format(filter(article_df,article_df$doc_id == all_doc[[1]][j])['published'][[1]],'%d-%m-%Y'),
                                            '\n',
                                            #location
                                            'LOCATION: ',
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['location'][[1]],
                                            '\n',
                                            '\n',
                                            #text
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['text'][[1]],
                                            '\n'
                                            )
  }
}

datatable(article_df_repeats[,c(1,5:10)], 
          class = 'cell-border stripe',
          caption = 'TABLE 3. Articles with identical titles for inspection',
          options = list(pageLength = 1,
                         height = 450,
                         width = 800),
          rownames = FALSE,
          colnames = c("Unique title",
                       "First article",
                       "Second article",
                       "Third article",
                       "Fourth article",
                       "Fifth article",
                       "Sixth article")) %>%
    formatStyle(1:5, 'vertical-align'='top') %>% 
    formatStyle(1:5, 'text-align' = 'left') 
```

```{r,eval = F, echo=F}
#export data for manual cleaning
write.csv(article_df_repeats[,c(1,5:10)],"article_compare.csv")
```

Briefly, from comparing some of the text in the above table, it appears that Petrus Gerhard is recycling articles that he writes for _Homeland Illumination_ for _All News Today_ (simply shortening the article and changing one or two words for _All News Today_), and often on the same day. Marcella Trapani and Eva Thayer are doing similar at _International News_ while officially writing for _Kronos Star_. There are many other cases, indicating that this "Shadow Author" phenomenon is probably much more widespread than previously expected. This warrants deeper investigation!!

To detect "Shadow Author" relationships, we will borrow a page from plagarism checker algorithms. We will use a simple substring matching approach to carry out this analysis. Simply put, the method involves the following:

1. **Breakdown each document into a series of n-grams.** n should be a medium length, ~5 to 10 words. Too long, and it won't capture situations where authors change one or two words. Too short, and we would miss word patterns. For this case, we choose n = 6.
2. **Calculate a similarity score**, where 1 indicates that the entirety of document A is found in document B. The score is given by:
$$
similarity (\text{doc A to doc B}) = \frac{\text{no. of n-grams in doc A found in doc B}}{\text{total number of n-grams in doc A}}
$$
3. Noting that the score is directional (i.e. similarity of A to B may be 1, but if B is a longer document then similarity of B to A will be < 1), we should designate the shorter document (higher similarity score) as the derivative source and the longer document (lower similarity score) as the primary. Not all forms of "Shadow Author" relationships will be so neat. From our manual review of the articles in the above sections, it is clear that some shadow authors "mix and match" paragraphs around. For simplicity, we will define a "Shadow Author" relationship based on the following:
    + **Links between documents are defined if at least one of the similarity scores is >0.5.** This allows for some coincidental overlaps in n-grams. >0.5 similarity score means more than half of the n-grams in doc A can be found in doc B, and indicates that one of the articles might have been "shadow authored".
    + **Document with the higher score will be designated as the derivative.**
    + **Document with the lower score will be designated as the primary.**
    + Where multiple documents are linked (similarity score >0.1), the **document with the lowest average similarity score will be designated as the primary source**, and all others will be designated as derivative.

```{r}
#breakdown the article text into n-grams 'fingerprints'
ngram_fp <- article_df[,c('doc_id','text')] %>%
  unnest_tokens(n-gram,text, token = "ngrams", n = 6)

ngram_fp$count = 1

#create 'n-gram'-document matrix
ndm <-ngram_fp %>%
  cast_tdm(n-gram,doc_id,count)

#calculate the similarity score
fp_similarity_mat <- crossprod_simple_triplet_matrix(ndm)/col_sums(ndm)

#create a reference similarity matrix
fp_similiarity_mat_ref <- fp_similarity_mat

```

The raw similiarity scores are not quite useful yet, we need to extract insight by determining which are the primary and derivative sources. 

One interesting point to note, is the the n-gram method will be extremely useful in helping us to identify primary sources for the quoting relationships. This is because a quote from another article will show up as an n-gram similarity score (i.e. a few sentences or paragraphs matching the primary source). If the similarity score is very close to 1, then the article may be a reprint of an older article.

We incoprporate this insight into our analysis and use the n-gram similarity score to fill in the primary sources for our quoting relationships. A single article may quote multiple sources, so the rules for defining primary and derivative source is not the same as those for shadow authoring. We will instead check off all articles from the quoted news source that have some similarity with the derivative article (i.e. the article doing the quoting).

```{r}
#utility code to reset the edge_list_quote
edge_list_quote$primary_doc_id <- ''

#clean up the matrix, we only want real relationships, not coincidental matches
for (i in 1:ncol(fp_similarity_mat)){
  #sparse matrix, so only zoom in on those with similarity score
  for (j in which(fp_similarity_mat[,i] >0)){
    #check if the doc has quoted another article
    if (rownames(fp_similarity_mat)[j] %in% unique(edge_list_quote$derivative_doc_id)){
      #check if the corresponding article source matches the quote source, if not, then no relationship
      #print(paste('Column: ',colnames(fp_similarity_mat)[i]))
      #print(paste('Row: ',rownames(fp_similarity_mat)[j]))
      if (article_df[colnames(fp_similarity_mat)[i],][2][[1]] == edge_list_quote[which(edge_list_quote$derivative_doc_id == rownames(fp_similarity_mat)[j])[1],][2][[1]]){
        #if so, update the edge_list_quote with the primary source doc_id,
        #check if there's already a doc_id in there, sometimes quote more than one source
        if (edge_list_quote[which(edge_list_quote$derivative_doc_id == rownames(fp_similarity_mat)[j])[1],'primary_doc_id'] != ''){
          new_row <- c(colnames(fp_similarity_mat)[i],
                       edge_list_quote$primary_source[which(edge_list_quote$derivative_doc_id == rownames(fp_similarity_mat)[j])[1]],
                       edge_list_quote$derivative_doc_id[which(edge_list_quote$derivative_doc_id == rownames(fp_similarity_mat)[j])[1]],
                       edge_list_quote$derivative_source[which(edge_list_quote$derivative_doc_id == rownames(fp_similarity_mat)[j])[1]],
                       #if similarity score is high, it means doc A is a reprint of doc B
                       ifelse(fp_similarity_mat[j,i] > 0.7,'Reprint','Quoting'))
          edge_list_quote <- rbind(edge_list_quote,new_row)
          } else {
            edge_list_quote[which(edge_list_quote$derivative_doc_id == rownames(fp_similarity_mat)[j]),'primary_doc_id'] <- colnames(
              fp_similarity_mat)[i]
            edge_list_quote[which(edge_list_quote$derivative_doc_id == rownames(fp_similarity_mat)[j]),'type'] <- ifelse(
              fp_similarity_mat[j,i] > 0.7,'Reprint','Quoting')
          }
          #no longer any need to reflect similarity if source is quoted, as it is now a "shadow" situation 
          fp_similarity_mat[j,i] <- 0
          fp_similarity_mat[i,j] <- 0
        } else {
          #if not a quoting relationship, check if the score is within range
          if ((fp_similarity_mat[j,i] > 0) & (fp_similarity_mat[j,i] < 0.5)){
            #check if the inverse similarity is similarly low
            #if both are low, link is coincidental, ignore by reducing both to 0
            if (fp_similarity_mat[i,j] < 0.5){
              fp_similarity_mat[j,i] <- 0
              fp_similarity_mat[i,j] <- 0
            }
          }
        }
      } else {
        #if not in list of articles that have quoted before, then go straight to inverse check
        #check is score is within range
        if ((fp_similarity_mat[j,i] > 0) & (fp_similarity_mat[j,i] < 0.5)){
          #check if the inverse similarity is similarly low
          #if both are low, link is coincidental, ignore by reducing both to 0
          if (fp_similarity_mat[i,j] < 0.5){
            fp_similarity_mat[j,i] <- 0
            fp_similarity_mat[i,j] <- 0
          }
        }
      }
  }
}
```
```{r}
#now we extract the relationships and label them, only the primary source's row will be labelled with a "1" for the corresponding derivative sources.
#first, copy the similarity matrix

fp_sim_copy <- fp_similarity_mat

for (i in 1:nrow(fp_sim_copy)){
  col_indices <- which(fp_similarity_mat[i,] >0)
  if (length(col_indices)>1){
    col_sums <- rep('', length(col_indices))
    for (j in 1:length(col_indices)){
      col_sums[j] <- sum(fp_similarity_mat[,col_indices[j]])-1
    }
    #doc A (row) in doc B (col) is the way the fp_similarity_mat is set out
    #so the maxcol sum indicates the doc B in which most doc As are found
    #and hence the primary source
    #this checks if the current row is a primary source
    if (col_indices[which.max(col_sums)] == i){
    #if yes, then we erase all the other links  
      for (k in 1:length(col_indices)){
        #all links should be zero
        for (m in 1:length(col_indices)){
          fp_sim_copy[col_indices[m],col_indices[k]] <- 0
        }
        #mark with 1 on the primary source line
        fp_sim_copy[i,col_indices[k]] <- 1
      } 
    }
  }
}
#remove all the self similarities
for (i in 1:nrow(fp_sim_copy)){
  fp_sim_copy[i,i] <- 0
}

#extract only rows and cols with some marker in them
fp_sim_copy <- fp_sim_copy[which(row_sums(fp_sim_copy)>0),which(col_sums(fp_sim_copy)>0)]

#convert matrix to dataframe for pivoting
fp_sim_copy <- as.data.frame(fp_sim_copy)

#extract the rownames as a column for primary doc_id
fp_sim_copy$primary_doc_id <- rownames(fp_sim_copy)

#create the edge list for shadow authors
edge_list_shadow <- fp_sim_copy %>%
  pivot_longer(cols = !primary_doc_id,
               names_to = 'derivative_doc_id') %>%
  filter(value == 1)

#remove the 'value' column
edge_list_shadow <- edge_list_shadow[,c(1:2)]

#add the additional edge details
edge_list_shadow$primary_source <- ''
edge_list_shadow$derivative_source <- ''

for (i in 1:nrow(edge_list_shadow)){
  #extract primary news source from article_df
  edge_list_shadow$primary_source[i] <- article_df[edge_list_shadow$primary_doc_id[i],][2][[1]]
  #extract derivative news source from article_df
  edge_list_shadow$derivative_source[i] <- article_df[edge_list_shadow$derivative_doc_id[i],][2][[1]]
}

edge_list_shadow$type <- 'Shadow Authoring'

#reorder the columns
edge_list_shadow <- edge_list_shadow[,c(1,3,2,4,5)]

#remove shadow authoring relationships where the primary and derivative news sources are the same
#self referencing within a news source should not be considered shadow authoring
edge_list_shadow <- edge_list_shadow[-which(edge_list_shadow$primary_source == edge_list_shadow$derivative_source),]
```

Having completed the edge list for "Shadow Authoring" relationships, we now turn our attention to tidying up the "Quoting" edge list. We then merge together the edge lists and prepare to generate a Sankey diagram to visualise the relationships.

```{r}
#tidy up the edge list and prepare graphics
#remove the remaining article in edge list quote with no primary source, 
#it might be a report about another news agency rather than a quote
edge_list <- rbind(edge_list_quote[-which(edge_list_quote$primary_doc_id == ''),],edge_list_shadow)
```

```{r layout="l-body-outset", fig.width = 20}
#expand edge list with article details
#add the additional edge details
edge_list$primary_year <- ''
edge_list$primary_author <- ''
edge_list$derivative_year <- ''
edge_list$derivative_author <- ''
edge_list$similarity <- ''

for (i in 1:nrow(edge_list)){
  #extract primary news source from article_df
  edge_list$primary_year[i] <- year(article_df[edge_list$primary_doc_id[i],][6][[1]])
  edge_list$primary_author[i] <- article_df[edge_list$primary_doc_id[i],][4][[1]]
  #extract derivative news source from article_df
  edge_list$derivative_year[i] <- year(article_df[edge_list$derivative_doc_id[i],][6][[1]])
  edge_list$derivative_author[i] <- article_df[edge_list$derivative_doc_id[i],][4][[1]]
  #extract the similarity score
  edge_list$similarity[i] <- fp_similiarity_mat_ref[which(rownames(fp_similiarity_mat_ref) == edge_list$derivative_doc_id[i]),
                                                 which(colnames(fp_similiarity_mat_ref) == edge_list$primary_doc_id[i])]
  
  #if primary and derivative author are not null and are the same, the author is not hiding
  #change relationship to reprint
  if ((edge_list$primary_author[i] != '')){
    if (edge_list$primary_author[i] == edge_list$derivative_author[i]){
      edge_list$type[i] = 'Reprint'
    }
  } else {
    #if the primary has no author and the derivative has an author, then the shadow authoring relationship should be inverted
    #the author could have added details in the shadow article not included in the original
    if (edge_list$derivative_author[i] != ''){
      #store the primary values
      derivative_doc_id <- edge_list$primary_doc_id[i]
      derivative_source <- edge_list$primary_source[i]
      derivative_year <- edge_list$primary_year[i]
      derivative_author <- edge_list$primary_author[i]
      
      #set primary values as the previously derivative values
      edge_list$primary_doc_id[i] <- edge_list$derivative_doc_id[i]
      edge_list$primary_source[i] <- edge_list$derivative_source[i]
      edge_list$primary_year[i] <- edge_list$derivative_year[i]
      edge_list$primary_author[i] <- edge_list$derivative_author[i]
      
      #set the derivative values as the previously primary values
      edge_list$derivative_doc_id[i] <- derivative_doc_id
      edge_list$derivative_source[i] <- derivative_source
      edge_list$derivative_year[i] <- derivative_year
      edge_list$derivative_author[i] <- derivative_author
    }
  }
}

#drop those where primary year > derivative year
edge_list <- edge_list[-which(edge_list$primary_year > edge_list$derivative_year),]

datatable(edge_list[,c(1:2,6:7,3:4,8:9,5,10)], 
          class = 'cell-border stripe',
          caption = 'TABLE 4. Full list of primary and derivative articles',
          options = list(pageLength = 10,
                         height = 450,
                         width = 800),
          rownames = FALSE,
          colnames = c("Primary doc ID",
                       "Primary News Source",
                       "Primary Publication Year",
                       "Primary Author",
                       "Derivative doc ID",
                       "Derivative News Source",
                       "Derivative Publication Year",
                       "Derivative Author",
                       "Type of Relationship",
                       "Similarity Score"))

```

We then prepare the data for visualisation.

```{r}
#summarise data by news source
edge_list_bysrc <- edge_list %>%
  group_by(primary_source,derivative_source,type) %>%
  summarise(freq = n())

#summarise data by author
edge_list_byauth <- edge_list %>%
  group_by(primary_author,derivative_author,type) %>%
  summarise(freq = n())

#split data before and after 2005
edge_list_bysrc_bef2005 <- edge_list %>%
  filter(derivative_year < 2005) %>%
  group_by(primary_source,derivative_source,type) %>%
  summarise(freq = n())

edge_list_bysrc_aft2005 <- edge_list %>%
  filter(derivative_year > 2004) %>%
  group_by(primary_source,derivative_source,type) %>%
  summarise(freq = n())
```

### 4.1.2. Answer

```{r echo=F}
#define nudge vector for labelling
nudge1 <- rep(-1,length(unique(edge_list$primary_source)))
nudge2 <- rep(1,length(unique(edge_list$derivative_source)))

nudge <- c(nudge1,nudge2)

#define the colours for the relationship type
quote <- "deepskyblue3"
reprint <- "gold2"
shadow <- "firebrick3"

#generate the sankey
ggplot(edge_list_bysrc,
       aes(y = freq, axis1 = primary_source, axis2 = derivative_source)) +
  geom_alluvium(aes(fill = type), 
                width = 1/12,
                alpha = 0.3, 
                knot.pos = 0.3) +
  geom_stratum(width = 1/12, fill = "grey50", color = "white") +
  #geom_label(stat = "stratum", aes(label = after_stat(stratum))) +
  scale_fill_manual('Type of Relationship',values  = c(quote,reprint,shadow)) +
  scale_color_manual(values = c(quote,reprint,shadow)) +
  scale_x_discrete(expand = c(.6, 0),
                   limits = c("Primary", "Derivative")) +
  ggrepel::geom_text_repel(
    stat = "stratum", 
    aes(label = after_stat(stratum)),
    size = 3, 
    direction = "y", 
    nudge_x = nudge
  ) +
  ggtitle("FIGURE 1. Primary and Derivative Relationships by News Source and Type") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, face = "bold"),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        legend.position = 'bottom')
```
From Figure 1 above, it can be seen that the bulk of the similarity between sources is due to "Shadow Authoring", where authors write for multiple news organisations without using their names. It is perhaps not surprising, given that Kronos is a very small country with a population of only 783,305 people, but supporting a media industry of 29 different news sources. From Figure 1, it appears that _News Online Today_ is the biggest beneficiary of such shadow authoring, with primary sources ranging from _Homeland Illumination_, _International Times_, _Kronos Star_, _The Abila Post_, and _The World_. _News Online Today_ appears to also be the largest single news agency in Kronos by article volume (see Figure 2 below), which probably indicates that they may also be one of the mainstream news sources in Kronos, which is why they are attractive to shadow writers. Some sources like _All News Today_ (linked to _Homeland Illumination_), _Central Bulletin_ (linked to _The Abila Post_), _International News_ (linked to _Kronos Star_), _Who What News_ (linked to _The World_), and _World Source_ (linked to _International Times_) derive almost all their derivative articles from a single source. This indicates that there might be some links between the reporting staff at those agencies and the primary source news agencies.

```{r, echo = F}
source_vol <- article_df %>%
  group_by(source) %>%
  summarise(count = n())

ggplot(source_vol,
       aes(x = source,
           y = count)) +
  geom_bar(stat = 'identity',
           fill = 'deepskyblue4') +
  ggtitle('FIGURE 2. Article Volume of various News Agencies') +
  ylab('Article Count') +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title.x = element_blank())
```
Assessing the types of relationships by author (see Figure 3 below), we find that most of the authors exclusively use shadow authoring, with only Petrus' articles being reprinted with his name in other sources. Note that there are many articles without author names.

```{r, echo = F}
#define nudge vector for labelling
nudge1 <- rep(-1,length(unique(edge_list$primary_author)))
nudge2 <- rep(1,length(unique(edge_list$derivative_author)))

nudge <- c(nudge1,nudge2)

#generate the sankey
ggplot(edge_list_byauth,
       aes(y = freq, axis1 = primary_author, axis2 = derivative_author)) +
  geom_alluvium(aes(fill = type), 
                width = 1/12,
                alpha = 0.3, 
                knot.pos = 0.3) +
  geom_stratum(width = 1/12, fill = "grey50", color = "white") +
  scale_fill_manual('Type of Relationship',values  = c(quote,reprint,shadow)) +
  scale_color_manual(values = c(quote,reprint,shadow)) +
  scale_x_discrete(expand = c(.6, 0),
                   limits = c("Primary", "Derivative")) +
  ggrepel::geom_text_repel(
    stat = "stratum", 
    aes(label = after_stat(stratum)),
    size = 3, 
    direction = "y", 
    nudge_x = nudge
  ) +
  ggtitle("FIGURE 3. Primary and Derivative Relationships by Author and Type") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, face = "bold"),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        legend.position = 'bottom')
```

One possible reason for this reluctance to explicitly declare their authorship could be that authors are deliberatly attempting to use a more reputable news source (e.g. News Online Today) to spread their message without compromising said news source by associating openly with it. Since 2005 (based on materials provided), the Protectors of Kronos (POK) have gotten increasingly militant and government crackdowns have similarly stepped up. Considering the political situation, we can compare the evolving dynamics of primary-derivative source relationships before 2005 and after 2005. From Figures 4a and 4b below, we can see that while shadow authoring relationships have always been prevalent, after 2005, no authors have published articles in other news sources under their own names any more.

```{r echo=F}
#define nudge vector for labelling
nudge1 <- rep(-1,length(unique(edge_list_bysrc_bef2005$primary_source)))
nudge2 <- rep(1,length(unique(edge_list_bysrc_bef2005$derivative_source)))

nudge <- c(nudge1,nudge2)

#generate the sankey
ggplot(edge_list_bysrc_bef2005,
       aes(y = freq, axis1 = primary_source, axis2 = derivative_source)) +
  geom_alluvium(aes(fill = type), 
                width = 1/12,
                alpha = 0.3, 
                knot.pos = 0.3) +
  geom_stratum(width = 1/12, fill = "grey50", color = "white") +
  scale_fill_manual('Type of Relationship',values  = c(quote,reprint,shadow)) +
  scale_color_manual(values = c(quote,reprint,shadow)) +
  scale_x_discrete(expand = c(.6, 0),
                   limits = c("Primary", "Derivative")) +
  ggrepel::geom_text_repel(
    stat = "stratum", 
    aes(label = after_stat(stratum)),
    size = 3, 
    direction = "y", 
    nudge_x = nudge
  ) +
  ggtitle("FIGURE 4a. Primary and Derivative Relationships by News Source and Type \n(before 2005)") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, face = "bold"),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        legend.position = 'top')
```
```{r echo=F}
#define nudge vector for labelling
nudge1 <- rep(-1,length(unique(edge_list_bysrc_aft2005$primary_source)))
nudge2 <- rep(1,length(unique(edge_list_bysrc_aft2005$derivative_source)))

nudge <- c(nudge1,nudge2)

#generate the sankey
ggplot(edge_list_bysrc_aft2005,
       aes(y = freq, axis1 = primary_source, axis2 = derivative_source)) +
  geom_alluvium(aes(fill = type), 
                width = 1/12,
                alpha = 0.3, 
                knot.pos = 0.3) +
  geom_stratum(width = 1/12, fill = "grey50", color = "white") +
  guides(fill = 'none') +
  scale_fill_manual('Type of Relationship',values  = c(shadow)) +
  scale_color_manual(values = c(shadow)) +
  scale_x_discrete(expand = c(.6, 0),
                   limits = c("Primary", "Derivative")) +
  ggrepel::geom_text_repel(
    stat = "stratum", 
    aes(label = after_stat(stratum)),
    size = 3, 
    direction = "y", 
    nudge_x = nudge
  ) +
  ggtitle("FIGURE 4b. Primary and Derivative Relationships by News Source and Type \n(after 2005)") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, face = "bold"),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())
```


## 4.2. QUESTION 2
_Characterize any biases you identify in these news sources, with respect to their representation of specific people, places, and events. Give examples. Please limit your answer to 6 images and 500 words._

## 4.2.1. Solution Approach and Data Wrangling

Visualising bias will require more in depth analysis of the article texts. We will used frequency-based analysis to characterise the articles for bias detection. First, we need to identify entities which we wish to study, namely:

1. Protectors of Kronos
2. Kronos Government
3. GAStech

We will define some keywords to label our dataset accordingly. Articles will be indicated as making a representation about each of these entities if certain keywords are mentioned in the article.

```{r}
#define key words
entities <- c('POK','Govt','GAStech')

keywords1 <- c('Protectors of Kronos','POK','Elian Karel',
               'Silvia Marek','Mandor Vann','Isia Vann',
               'Lucio Jakab','Lorenzo Di Stefano')
keywords2 <- c('government', 'ministry', 'minister', 
               'president', 'military', 'police',
               'mayor')
keywords3 <- c('GAStech')

keywords <- list(keywords1,keywords2,keywords3)

article_df[entities] <- NA

for (i in 1:nrow(article_df)){
  text <- str_to_lower(gsub("[[:punct:]]", "", article_df$text[i]))
  for (j in 1:length(keywords)){
    for (k in 1:length(keywords[[j]])){
      if (str_detect(text,str_to_lower(keywords[[j]][k]))){
        article_df[i,7+j] <- TRUE
      } else {
        if (isNA(article_df[i,7+j])){
          article_df[i,7+j] <- FALSE
        }
      }
    }
  }
}
```

Now, we turn our attention to the sentiment analysis.

First, we will convert our corpus of articles into a document-term matrix, using [text frequency - inverse document frequency](https://www.tidytextmining.com/tfidf.html) (TF-IDF) to fill in the values. TF-IDF is used because it supresses words that appear commonly across all documents through the inverse document frequency term. This eliminates the need to specify stopwords to remove from the corpus, which is tedious and might not be too accurate as some stopwords also depend on the unique characteristics of each corpus. 

We will also lemmatize the words using the [textstem](https://rdrr.io/cran/textstem/) package to better capture the meaning of each document by converting various forms of words to their root word (e.g. 'running', 'ran', and 'run' will be reduced to 'run'). Another approach is stemming, where t   he ends of words are truncated based on heuristics to remove derivational and inflectional affixes (e.g. -s, -ed, -es, -ing, -tion). Past research ([di Nanzio, Vezzani, 2018](http://ceur-ws.org/Vol-2253/paper45.pdf)) has shown that lemmatization and stemming are comparable in terms of performance. We will go with lemmatization for this project because the output are base words, which are more understandable.

We will also remove numerics so that we can avoid cluttering the document-term matrix and zoom in on word meanings to analyse sentiment.

```{r}
dtm <- article_df[,c('doc_id','text')] %>%
  unnest_tokens(word,text) 

#lemmatize words
dtm$word <- lemmatize_words(dtm$word)

#establish word counts
dtm <- dtm %>%
  count(doc_id, word, sort=TRUE)

#remove numerics
dtm <- dtm[-which(grepl("^[[:digit:]]+", dtm$word)),]

#calculate tf-idf
dtm <- dtm %>%
  bind_tf_idf(word,doc_id, n)

#cast dtm
dtm <-dtm %>%
  cast_dtm(doc_id,word,tf_idf)
```

We plot a wordcloud (see Figure 5 below) to compare the common words used when each of the three entities are mentioned in an article. From the word cloud, it can be seen that the words commonly used when talking about the government or GAStech are largely neutral, while those used in describing POK have more negative connotations (e.g. violence, vandalism, assault).

```{r, echo=F, layout="l-body-outset", fig.width = 5}
batch1 <- col_means(dtm[which(rownames(dtm) %in% article_df$doc_id[article_df$POK]),])
batch2 <- col_means(dtm[which(rownames(dtm) %in% article_df$doc_id[article_df$Govt]),])
batch3 <- col_means(dtm[which(rownames(dtm) %in% article_df$doc_id[article_df$GAStech]),])

batch <- rbind(batch1,
               batch2,
               batch3)

rownames(batch) <- entities

layout(matrix(c(1, 2), nrow=2), heights=c(1, 25))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, 
     y=0.5,
     cex = 0.6,
     "FIGURE 5. Comparison wordcloud for various entity mentions")
comparison.cloud(t(batch),
                 max.words=300,
                 random.order=FALSE,
                 rot.per=0.35,
                 scale = c(2, 0.22),
                 title.size = 1.5,
                 colors = brewer.pal(8, "Dark2"))

```
However, we now need to characterise the individual words in order to capture sentiment, so that we can apply an analysis on each news source to identify bias. Our approach will be as follows:

1. **Extract and manually annotate the vocabulary** to characterise words as follows:
    - Negative to POK (e.g. terrorist, vandalism, riot, loot, criminal)
    - Positive to POK (e.g. activist, protest, demonstration, peaceful)
    - Negative to Government (e.g. favoritism, corruption, kleptocracy)
    - Positive to Government (e.g. investment, development)
    - Negative to GAStech (e.g. toxic, acidic, contamination)
2. For each set of entity mentions:
    - **Summarise the document-term matrix by news source**, taking the column means to get the average sentiment of the articles produced by the news source mentioning said entity.
    - **Multiply the td-idf score for each word by the annotated word sentiment score** (-1 for negative, 0 for neutral, 1 for positive).
    - **Sum the sentiment scores for all words for each news source.** That will be the sentiment score for the news source.
    - **Normalise the sentiment scores, by computing the z-statistic.** This is to ensure that the relative favorability of a news source for each entity is comparable across entities
3. **Visualise the relative favorability of each news source with respect to each entity.**

```{r, eval = F}
#extract vocabulary, only interested in words with high tf-idf scores
vocabulary <- c()

for (i in 1:ncol(dtm)){
  if (max(dtm[,i])>0.05){
    vocabulary <- c(vocabulary,colnames(dtm)[i])
  }
}

#export vocabulary for manual annotation
write.csv(vocabulary,"vocab.csv")
```

After manual annotation, we will create a reference dictionary with the sentiment tagged to each word for data wrangling.

```{r}
#load in annotated vocabulary
vocab_annotated <- read.csv('vocab_annotated.csv')

#remove old index
vocab_annotated <- vocab_annotated[,2:5]

#convert NA to 0
vocab_annotated[is.na(vocab_annotated)] <- 0 

#remove neutral words
vocab_annotated <- vocab_annotated[which(row_sums(vocab_annotated[,2:4]) != 0),]

#create a reference dictionary
ref_dict <- Terms(dtm)

#populate the ref dictionary
for (i in 1:length(entities)){
  new_row <- rep(0,length(ref_dict))
  if (i == 1){
    for (j in 1:nrow(vocab_annotated)){
      new_row[which(ref_dict == vocab_annotated[j,1])] <- vocab_annotated[j,i+1] 
    }
  } else {
    for (j in 1:nrow(vocab_annotated)){
      new_row[which(colnames(ref_dict) == vocab_annotated[j,1])] <- vocab_annotated[j,i+1] 
    }
  }
  ref_dict <- rbind(ref_dict,new_row)
  #set column names to be the terms
  colnames(ref_dict) <- Terms(dtm)
}

#remove the first row
ref_dict <- ref_dict[-1,]

#set row names to be the entities
rownames(ref_dict) <- entities

```
We then compute the sentiment scores for each news source with respect to each entity.

```{r}
#first create subset dtms for each group of articles
dtm_pk <- dtm[which(rownames(dtm) %in% article_df$doc_id[article_df$POK]),]
dtm_gv <- dtm[which(rownames(dtm) %in% article_df$doc_id[article_df$Govt]),]
dtm_ga <- dtm[which(rownames(dtm) %in% article_df$doc_id[article_df$GAStech]),]

#define function to summarise the dtm by news source (need to reference from the article_df table)
summarise_bysrc <- function(dtm){
  for (i in 1:length(news_sources)){
    if (i == 1){
      dtm_bysrc <- col_means(dtm[which(rownames(dtm) %in% article_df$doc_id[which(article_df$source == news_sources[i])]),])
    } else {
      dtm_bysrc <- rbind(dtm_bysrc,
                         col_means(dtm[which(rownames(dtm) %in%
                                               article_df$doc_id[which(article_df$source == news_sources[i])]),]))
    }
  }
  #set the col and row names
  colnames(dtm_bysrc) <- Terms(dtm)
  rownames(dtm_bysrc) <- news_sources
  
  return(dtm_bysrc)
}

#perform summaries by news source
dtm_pk_bysrc <- summarise_bysrc(dtm_pk)
dtm_gv_bysrc <- summarise_bysrc(dtm_gv)
dtm_ga_bysrc <- summarise_bysrc(dtm_ga)

sent_scores <- data_frame(news_source = news_sources,
                          POK = rep(0,length(news_sources)),
                          Govt = rep(0,length(news_sources)),
                          GAStech = rep(0,length(news_sources)))

#define function to perform element-wise multiplication with the sentiment vocab
get_sentiment <- function(dtm,ent,sent_scores){
  for (i in 1:nrow(dtm)){
    sent_scores[i,ent+1] <- sum(dtm_pk_bysrc[i,]*as.numeric(ref_dict[ent,]))
  }
  return(sent_scores)
}

#perform element-wise multiplication with the sentiment vocab
sent_scores <- get_sentiment(dtm_pk_bysrc,1,sent_scores)
sent_scores <- get_sentiment(dtm_gv_bysrc,2,sent_scores)
sent_scores <- get_sentiment(dtm_ga_bysrc,3,sent_scores)

#normalise the sent_scores
for (i in 2:ncol(sent_scores)){
  mean <- mean(as.numeric(as.matrix(sent_scores)[,i]))
  sd <- sd(as.numeric(as.matrix(sent_scores)[,i]))
  for (j in 1:nrow(sent_scores)){
    sent_scores[j,i] <- (sent_scores[j,i] - mean)/(sd)
  }
}

#pivot for plotting
sent_scores <- pivot_longer(sent_scores,
                            cols = !news_source,
                            names_to = 'entity',
                            values_to = 'sent_score')

```

### 4.2.2. Answer

Based on the results in Figure 6 below, we find that most news sources are more neutral in stance (i.e. clustered around the centre of the sentiment ratings). In general there is greater variability in favorability for POK, while favorability ratings for the government tend to be clustered in a tighter band, indicating that most news sources tend to report objectively or slightly favorably on the government. This is perhaps to be expected, as the government is usually able to exert some influence over news media in a country. 

It is noted that _All News Today_ is an outlier. It is the most vehemently anti-government and anti-GAStech news source, while it is also the most pro-POK news source. This is not surprising, and _All News Today_ could have links with the POK organisation and senior leadership. We also note from the analysis in 4.2.1. that _All News Today_ derives alot of content from shadow authors (i.e. Petrus Gerhard and Maha Salo) in _Homeland Illumination_, which is also noted to be slightly negative towards the government and GAStech and slightly more positive towards POK. The shadow authors would be a good place to investigate for links between POK and the media.


```{r, echo=F}
#generate the jitter stripchart
ggplot(sent_scores, 
       aes(x = entity,
           y = sent_score)) +
  geom_jitter(position = position_jitter(0.03), 
              size = 3,
              alpha = 0.5,
              colour = 'deepskyblue4') +
  geom_hline(yintercept = 0,
             linetype = 'dashed',
             color = 'red')+
  geom_text(aes(3.5,
                2.5,
                label = 'More Favorable\nthan Average',
                color = 'red')) +
  geom_text(aes(3.5,
                -3,
                label = 'Less Favorable\nthan Average',
                color = 'green')) +
  scale_x_discrete(expand = c(.6, 0)) +
  ggrepel::geom_text_repel(
    stat = "identity", 
    aes(label = news_source),
    size = 3, 
    direction = "y",
    max.overlaps = 8,
    nudge_x = -0.5) +
  ggtitle('FIGURE 6. Relative favorability of News Sources with respect to Entity') +
  ylab('More Favorable >>') +
  xlab("Entity") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x = element_text(size = 10, face = "bold"),
        axis.title.x = element_text(size = 12, face = "bold"),
        legend.position = "none",
        text = element_text(size = 7, face = "bold"))
```
To validate our findings, let's perform a wordcloud of _All News Today's_ articles regarding each entity and see what kind of words they tend to use regarding each entity. From the word cloud, we can see that _All News Today_ tends to report alot on corruption and kleptocracy when reporting on GAStech or the government, while it tends to portray POK as a grassroots organisation supported by the people. This validates our findings from the sentiment analysis.

```{r, echo=F}
batch1 <- dtm_pk_bysrc['All News Today',]
batch2 <- dtm_gv_bysrc['All News Today',]
batch3 <- dtm_ga_bysrc['All News Today',]

batch <- rbind(batch1,
               batch2,
               batch3)

rownames(batch) <- entities

layout(matrix(c(1, 2), nrow=2), heights=c(1, 25))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, 
     y=0.5,
     cex = 0.6,
     "FIGURE 7. Comparison wordcloud for various entity mentions for All News Today")
comparison.cloud(t(batch),
                 max.words=200,
                 random.order=FALSE,
                 rot.per=0.35,
                 scale = c(2.5, 0.6),
                 title.size = 1.2,
                 colors = brewer.pal(8, "Dark2"))

```

## 4.3. QUESTION 3 
_Given the data sources provided, use visual analytics to identify potential official and unofficial relationships among GASTech, POK, the APA, and Government. Include both personal relationships and shared goals and objectives. Provide evidence for these relationships. Please limit your answer to 6 images and 400 words._

### 4.3.1. Solution Approach and Data Wrangling

We will do a simple graph analysis to explore relationships between GAStech employees using the email headers coupled with some employee data.

First, we will extract the email data for manual annotation.

```{r eval=FALSE}
write.csv(email_header_proc,'email_header_annotate.csv')
```

We reload the annotated data back in.

```{r}
edge_list <- read.csv('email_header_annotate.csv')

#wrangling time
edge_list$sent_date <- ymd(edge_list$sent_date)
edge_list$weekday <- wday(edge_list$sent_date,
                         label = TRUE,
                         abbr = FALSE)

#replace wrong email addresses
edge_list$source <- str_replace(edge_list$source,'tethys','kronos')
edge_list$target <- str_replace(edge_list$target,'tethys','kronos')

```

Now we will create nodes from the employee data.

```{r}
node_list <- employee_data[,c(1:3,5:6,12:15)]

#need to add the 19- to make sure lubridate doesn't read as 21st century
node_list$BirthDate <- str_replace(node_list$BirthDate, '-([0-9]{2}$)', '/19\\1')

#convert to date
node_list$BirthDate <- dmy(node_list$BirthDate)
node_list$CurrentEmploymentStartDate <- dmy(node_list$CurrentEmploymentStartDate)

#calculate age
node_list$age <- year(max(edge_list$sent_date)) - year(node_list$BirthDate)
node_list$years_employed <- year(max(edge_list$sent_date)) - year(node_list$CurrentEmploymentStartDate)

#drop birth date and employee start date
node_list <- node_list[,-c(3,8)]

#give an id number for graphing
node_list$id <- c(1:nrow(node_list))

#rearrange the columns
node_list <- node_list[,c(10,7,1,2,8,3,4,9,5,6)]

#rename the columns
colnames(node_list) <- c('id',
                         'email',
                         'last_name',
                         'first_name',
                         'age',
                         'gender',
                         'citizenship',
                         'years_employed',
                         'department',
                         'title')
```

We will also prepare data to make alluvial plots. There might be too many unique employees and titles to get a meaningful visualisation, but we shall prepare first for experimentation.

```{r}
#prepare the edgelist for alluvial plot
edge_list_alluv <- edge_list[,-1]

edge_list_alluv$source_name <- ''
edge_list_alluv$target_name <- ''
edge_list_alluv$source_dept <- ''
edge_list_alluv$target_dept <- ''
edge_list_alluv$source_age <- ''
edge_list_alluv$target_age <- ''
edge_list_alluv$source_employed <- ''
edge_list_alluv$target_employed <- ''
edge_list_alluv$source_title <- ''
edge_list_alluv$target_title <- ''

for (i in 1:nrow(edge_list_alluv)){
  edge_list_alluv$source_name[i] <- paste(node_list$last_name[which(node_list$email == edge_list_alluv$source[i])],
                                          ', ',
                                          node_list$first_name[which(node_list$email == edge_list_alluv$source[i])])
  edge_list_alluv$target_name[i] <- paste(node_list$last_name[which(node_list$email == edge_list_alluv$target[i])],
                                          ', ',
                                          node_list$first_name[which(node_list$email == edge_list_alluv$target[i])])
  edge_list_alluv$source_dept[i] <- node_list$department[which(node_list$email == edge_list_alluv$source[i])]
  edge_list_alluv$target_dept[i] <- node_list$department[which(node_list$email == edge_list_alluv$target[i])]
  edge_list_alluv$source_age[i] <- node_list$age[which(node_list$email == edge_list_alluv$source[i])]
  edge_list_alluv$target_age[i] <- node_list$age[which(node_list$email == edge_list_alluv$target[i])]
  edge_list_alluv$source_employed[i] <- node_list$years_employed[which(node_list$email == edge_list_alluv$source[i])]
  edge_list_alluv$target_employed[i] <- node_list$years_employed[which(node_list$email == edge_list_alluv$target[i])]
  edge_list_alluv$source_title[i] <- node_list$title[which(node_list$email == edge_list_alluv$source[i])]
  edge_list_alluv$target_title[i] <- node_list$title[which(node_list$email == edge_list_alluv$target[i])]
}

create_aggregated_alluv <- function(alluv,detail){
  targets <- list(c(8,9),
                  c(10,11),
                  c(12,13),
                  c(14,15),
                  c(16,17))
  alluv_agg <- alluv %>%
    group_by(type,
             source = alluv[,targets[detail][[1]][1]],
             target = alluv[,targets[detail][[1]][2]]) %>%
    summarise(freq = n())
  
  return(alluv_agg)
}

alluv_name <- create_aggregated_alluv(edge_list_alluv,1)
alluv_dept <- create_aggregated_alluv(edge_list_alluv,2)
alluv_age <- create_aggregated_alluv(edge_list_alluv,3)
alluv_employed <- create_aggregated_alluv(edge_list_alluv,4)
alluv_title <- create_aggregated_alluv(edge_list_alluv,5)
```

We need to merge the ID numbers back with the edge list to support the graphing.

```{r}
emails_id <- node_list[,1:2]

edge_list$from <- ''
edge_list$to <- ''

for (i in 1:nrow(edge_list)){
  edge_list$from[i] <- which(emails_id$email == edge_list$source[i])
  edge_list$to[i] <- which(emails_id$email == edge_list$target[i])
}

edge_list <- edge_list[,-c(1,3:4)]
  
edge_list <- edge_list %>% 
  rename(
    source = from,
    target = to
  )

#aggregating the edges by count
edge_list_aggregated_wr <- edge_list %>%
  filter(type == "Work related") %>%
  group_by(source, target, weekday) %>%
    summarise(weight = n()) %>%
  filter(source != target) %>%
  filter(weight > 1) %>%
  ungroup()

edge_list_aggregated_nwr <- edge_list %>%
  filter(type == "Non-work related") %>%
  group_by(source, target, weekday) %>%
    summarise(weight = n()) %>%
  filter(source != target) %>%
  filter(weight > 1) %>%
  ungroup()

```


### 4.3.2. Answer

```{r echo=F}
#define function to plot alluvial
plot_alluv <- function (df,det,fig){
  #define vector of details
  details <- c('Name',
               'Department',
               'Age',
               'Years Employed',
               'Title')
  
  #define nudge vector for labelling
  nudge1 <- rep(-1,length(unique(df$source)))
  nudge2 <- rep(1,length(unique(df$target)))

  nudge <- c(nudge1,nudge2)

  #generate the sankey
  ggplot(df,
         aes(y = freq, axis1 = source, axis2 = target)) +
    geom_alluvium(aes(fill = type), 
                width = 1/12,
                alpha = 0.3, 
                knot.pos = 0.3) +
    geom_stratum(width = 1/12, fill = "grey50", color = "white") +
    scale_fill_manual('Type of Email',values  = c('greenyellow','grey50')) +
    scale_color_manual(values = c('greenyellow','grey50')) +
    scale_x_discrete(expand = c(.6, 0),
                   limits = c("Sender", "Receiver")) +
    ggrepel::geom_text_repel(
      stat = "stratum", 
      aes(label = after_stat(stratum)),
      size = 3, 
      direction = "y", 
      nudge_x = nudge
    ) +
    ggtitle(paste('FIGURE ',
                  fig,
                  '. Email traffic by ',
                  details[det])) +
    theme_minimal() +
    theme(axis.text.x = element_text(size = 12, face = "bold"),
          axis.text.y = element_blank(),
          axis.title.y = element_blank(),
          legend.position = 'top')
}
```

With reference to Figure 8, we can see that most emails, whether work or non-work related are sent within a department. This indicates that employees tend to form social circles within their own departments, as is expected.

```{r, echo=F}
plot_alluv(alluv_dept,2,8)
```
Plotting a network graph to visualise the email relationships, we find very different relationships for the work-related (Figure 9) and non-work related (Figure 10) emails. For work related emails, the bulk of the emails are confined within the department. There are a few individuals who are at the centre of the network who send the most work emails across the organisation, namely the assistant to the CEO, and the assistant to the Engineering Group Manager.

```{r layout="l-body-outset",echo=F}
#visNetwork needs the column to be named from and to
edge_list_aggregated_wr_tf <- edge_list_aggregated_wr %>%
  rename(
    from = source,
    to = target
  )

#visNetwork needs to name the color coding column group
colnames(node_list)[9] <- 'group'

#visualise network
visNetwork(node_list,
           edge_list_aggregated_wr_tf,
           main = 'FIGURE 9. Network graph of work-related emails') %>%
  visIgraphLayout(layout = 'layout_with_fr') %>%
  visGroups(groupname = 'Administration', color = 'lightblue') %>%
  visGroups(groupname = 'Engineering', color = "green") %>%
  visGroups(groupname = 'Executive', color = 'gold') %>%
  visGroups(groupname = 'Facilities', color = "grey") %>%
  visGroups(groupname = 'Information Technology', color = 'pink') %>%
  visGroups(groupname = 'Security', color = "brown") %>%
  visLegend()

```

```{r layout="l-body-outset", echo=F}
#visNetwork needs the column to be named from and to
edge_list_aggregated_nwr_tf <- edge_list_aggregated_nwr %>%
  rename(
    from = source,
    to = target
  )

#visNetwork needs to name the color coding column group
colnames(node_list)[9] <- 'group'

#visualise network
visNetwork(node_list,
           edge_list_aggregated_nwr_tf,
           main = 'FIGURE 10. Network graph of work-related emails') %>%
  visIgraphLayout(layout = 'layout_with_fr') %>%
  visGroups(groupname = 'Administration', color = 'lightblue') %>%
  visGroups(groupname = 'Engineering', color = "green") %>%
  visGroups(groupname = 'Executive', color = 'gold') %>%
  visGroups(groupname = 'Facilities', color = "grey") %>%
  visGroups(groupname = 'Information Technology', color = 'pink') %>%
  visGroups(groupname = 'Security', color = "brown") %>%
  visLegend()

```