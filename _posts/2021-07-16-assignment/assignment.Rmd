---
title: "Assignment"
description: |
  My approach and solution to Mini Challenge 1 of the VAST Challenge 2021.
author:
  - name: Arnold Ng
    url: {}
date: 07-16-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy.opts=list(width.cutoff=40),
                      tidy=TRUE)
```

```{r echo=FALSE}
#load relevant libraries
packages = c('DT','ggiraph','plotly','tidyverse',
             'tidytext','readtext','lubridate','formatR',
             'textstem')

for (p in packages){
  if(!require(p,character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

setwd("~/0. SMU MITB/9. ISSS608 Visual Analytics & Applications/arnoldndx/data_viz_makeover/_posts/2021-07-16-assignment")
```

# 1. Task

The task is to use visual analytics to examine text data comprising:
* news articles; 
* GAStech employee email headers;
* GAStech employee records; and
* GAStech employee resumes

in order to answer the following questions.

**QUESTION 1:** Characterize the news data sources provided. Which are primary sources and which are derivative sources? What are the relationships between the primary and derivative sources? Please limit your answer to 8 images and 300 words.

**QUESTION 2:** Characterize any biases you identify in these news sources, with respect to their representation of specific people, places, and events. Give examples. Please limit your answer to 6 images and 500 words.

**QUESTION 3:** Given the data sources provided, use visual analytics to identify potential official and unofficial relationships among GASTech, POK, the APA, and Government. Include both personal relationships and shared goals and objectives. Provide evidence for these relationships. Please limit your answer to 6 images and 400 words.

# 2. Data Preparation

Firstly, we will need to clean up and process the data into data tables to facilitate analysis. We will use a combination of R tools and manual inspection/editing to clean up the data.

There are three main datasets that will need to be processed and tidied up. The first set comprises the news articles. They will have to be collected into a data table for analysis, with key article details such as title, author, publish date, etc. extracted to facilitate analysis. The second set of data comprises the employee relationships to each other, as well as entities such as countries, organisations, persons outside GAStech, etc. The third and final set of data comprises email headers for two weeks worth of emails, along with labels of the type of emails. 

## 2.1. News Articles

There is noise in the news article data, which includes incosistent line breaks, inconsistent formatting of article details such as dates, author names, etc., and inconsistent details provided per article. This has to be taken into account in processing the article data.

### **Step 2.1.1:** Combine articles into single table for checking

We first begin by combining the article data into a single table to facilitate checks. We get the easy part out of the way and clean up the line breaks, by replacing all double line breaks with single line breaks with the code below.

```{r eval=FALSE}
articles <- readtext("MC 1/News Articles/*")

for (row in 1:nrow(articles)){
  #clean up the text of breaks
  while (str_detect(articles[row,'text'],'\n ')){
    articles[row,'text'] <- str_replace_all(articles[row,'text'],"\n ","\n")
  }
  while (str_detect(articles[row,'text'],'\n\n')){
    articles[row,'text'] <- str_replace_all(articles[row,'text'],"\n\n","\n")
  }
}
```

We then export the data for manual cleaning (i.e. edit the text directly in the csv file).

```{r eval=FALSE}
#export data for manual cleaning
write.csv(articles,"article_consol.csv")
```

### **Step 2.1.2:** Load cleaned data and convert to article data into analysable dataframe

We now begin the task of converting the data into an analysable format. 

First, we perform a first pass manual check of all the article text and manually clean up the data into a consistent format.Then we load in the cleaned data.

```{r}
#load in cleaned data
article_data <- read.csv("article_consol_edit.csv")
```

Next, we convert the article data into an analysable dataframe to with the relevant article information separated out. We also ensure that the published dates are properly formatted for analysis.

```{r}
#create dataframe to store article data
article_df <- data_frame(doc_id = rep('',nrow(article_data)),
                         source = rep('',nrow(article_data)),
                         title = rep('',nrow(article_data)),
                         author = rep('',nrow(article_data)),
                         location = rep('',nrow(article_data)),
                         published = rep('',nrow(article_data)),
                         text = rep('',nrow(article_data)))

#define the article details to be captured
headers <- c('source',
             'title',
             'author',
             'location',
             'published')

for (row in 1:nrow(article_data)){
  article_text <- str_split(article_data$text[row],'\n')
  article_df$doc_id[row] <- article_data$doc_id[row]
  for (i in 1:length(article_text[[1]])){
    detail <- str_trim(str_split_fixed(article_text[[1]][i],":",2))
    # check if any of the string headers corresponds to article details, otherwise treat as body text
    if (tolower(detail[1]) %in% headers){
      #some of the article text has "LOCATION:", especially police blotter transcripts, this is to make sure only the first instance of location is used.
      if (article_df[row,tolower(detail[1])] == ''){
        #for multiple authors store authors as list so that they can be referenced separately
        if (tolower(detail[1]) == 'author'){
          article_df[row,tolower(detail[1])] <- list(str_trim(str_split(detail[2],',')))
        } else if (tolower(detail[1]) == 'published'){
          date <- str_replace_all(str_replace_all(detail[2],'  ',' '),',','')
          article_df[row,'published'] <- date
        }
        article_df[row,tolower(detail[1])] <- str_trim(detail[2])
      } else {
        #concatenate body text
        article_df[row,'text'] <- paste(article_df[row,'text'],article_text[[1]][i], sep='\n')
      }
    } else {
      #concatenate body text
      article_df[row,'text'] <- paste(article_df[row,'text'],article_text[[1]][i], sep='\n')
    }
  } 
}

#convert the published dates to dates
article_df$published <- as.Date(parse_date_time(article_df$published,c('dmy','mdy','ymd')))

```
And we're done with the news article data!

## 2.2. GAStech employee relationships

## 2.3. Email headers

# 3. Solutions (Approach & Answers)

Using the prepared data, we now answer the questions with the help of some meaningful visualisations.

## 3.1. Question 1
_Characterize the news data sources provided. Which are primary sources and which are derivative sources? What are the relationships between the primary and derivative sources? Please limit your answer to 8 images and 300 words._

### 3.1.1. Solution Approach and Data Wrangling

There are two ways to detect derivative sources:
1. presence of other news source name in the text (e.g. quoting or crediting another news source)
2. same title and author as another article (e.g. reprinting or republishing a previous article from another news source)

**STEP 1:** Identify presence of other news source name

For the first approach, we will establish a listing of all news sources, and then check each article text for the presence of other news sources. We will use the detects to generate an adjacency matrix to visualise the links between articles. The relationship will be defined as follows:
1. The primary source will be the news source quoted
2. The derivative source is the article doing the quoting
3. The relationship is defined as "Quoting"

```{r}
news_sources <- c(unique(article_df$source))

#append one column for each news source to article df to establish an adjacency matrix
article_df[news_sources] <- NA

#mark TRUE whenever mentions of other news sources are detected
for (row in 1:nrow(article_df)){
  for (news_source in news_sources){
    if (article_df[row,'source'] != news_source){
      article_df[row,news_source] <- str_detect(article_df[row,'text'], news_source)
    }
    else{article_df[row,news_source] <- FALSE}
  }
}
```

**STEP 2:** Identify articles with same title and author

First we compile a list of unique title-author combinations and extract the earliest publication of each combination. This list will be the basis for comparison against all the articles. Any articles published after the earliest publication is necessarily a reprint of the primary source. 

```{r}
#establish a list of unique article title/author combinations and record the earliest publish date and the source

article_df_grouped <-  article_df %>%
  group_by(title,author) %>%
  arrange(published, by_group = TRUE) %>%
  summarise(first_doc_id = first(doc_id), 
            published = first(published), 
            news_source = first(source),
            text = first(text),
            #the below details are for further analysis
            count = n(), #count the number of times the title-author combo been published
            all_source = list(unique(source)), #list the number of news sources that have used this article
            all_doc = list(doc_id)) #list all the documents that share this article-author combination

datatable(article_df_grouped[,c(1:5,7:9)],
          class = 'cell-border stripe',
          colnames = c("Unique title",
                       "Unique author",
                       "First Document ID",
                       "First published date",
                       "First published news source",
                       "Repeat count",
                       "All sources that have published this article",
                       "All articles with this title-author combination"))
```
**

One quick observation that we can make is that there are several instances where articles share titles (i.e. exact match) with one combination with an author's name indicated but another combination without an author's name. This indicates that there might be situations either where one source is plagarising another or authors with one publication are re-using their work with another publication without using their bylines. 

To further investigate this we shall extract the text of all articles which share the same title for comparison. We should note that if there is only one news source that has used this title (even if there are multiple articles), then this article could be a topic header for a developing story, with periodic updates being provided. In such a case, we ignore it as it does not show a primary-derivative source relationship.

```{r echo=FALSE}
#filter the article data to just those with identical titles, we will use this repeats table to extract the relevant article texts

article_df_repeats <- article_df %>%
  group_by(title) %>%
  summarise(count_articles = n(),
            num_unique_sources = length(list(unique(source))[[1]]), #count the number of news sources that have used this article title
            all_docs = list(doc_id) #list all the documents that share this article-author combination
            ) %>%
  filter(num_unique_sources > 1)

coladd <- ncol(article_df_repeats)

article_df_repeats[(coladd+1):(coladd+max(article_df_repeats$num_unique_sources))] <- ''

for (i in 1:nrow(article_df_repeats)){
  all_doc <- article_df_repeats$all_docs[i]
  for (j in 1:length(all_doc[[1]])){
    article_df_repeats[i,coladd+j] <- paste('DOC_ID: ',
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['doc_id'],
                                            '\n',
                                            #source
                                            'SOURCE: ',
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['source'],
                                            '\n',
                                            #author
                                            'AUTHOR: ',
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['author'],
                                            '\n',
                                            #published date
                                            'PUBLISHED: ',
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['published'],
                                            '\n',
                                            #location
                                            'LOCATION: ',
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['location'],
                                            '\n',
                                            '\n',
                                            #text
                                            filter(article_df,article_df$doc_id == all_doc[[1]][j])['text'],
                                            '\n'
                                            )
  }
}

datatable(article_df_repeats[,c(1,5:10)], 
          class = 'cell-border stripe',
          options = list(pageLength = 5,
                         height = 450),
          colnames = c("Unique title",
                       "First article",
                       "Second article",
                       "Third article",
                       "Fourth article",
                       "Fifth article",
                       "Sixth article"))
```

```{r echo=FALSE}
#filter the article data to just those with identical titles

article_df_repeats <- article_df_grouped %>%
  group_by(title) %>%
  summarise(title = title,
            text = text,
            author = author,
            news_source = news_source,
            published = published
            ) %>%
  filter(n()>1)

datatable(article_df_repeats, 
          class = 'cell-border stripe',
          options = list(pageLength = 2))
```
Briefly, from comparing some of the text in the above table, it appears that Petrus Gerhard is recycling articles that he writes for _Homeland Illumination_ for _All News Today_ (simply shortening the article and changing one or two words for _All News Today_), and often on the same day. Marcella Trapani and Eva Thayer are doing similar at _International News_ while officially writing for _Kronos Star_. In these cases, we will record the relationship between the articles as follows:
1. The primary source is the longer article which bears the author's name.
2. The derivative source is the shorter article which is published without an author specified.
3. The relationship is defined as "Shadow Author"




### 3.1.2. Answer


## 3.2. QUESTION 2
_Characterize any biases you identify in these news sources, with respect to their representation of specific people, places, and events. Give examples. Please limit your answer to 6 images and 500 words._

##3.2.1. Solution Approach and Data Wrangling

Visualising bias will require more in depth analysis of the article texts. We will used frequency-based analysis to characterise the articles.

```{r}
#define key words


```

Firstly, we will convert our corpus of articles into a document-term matrix, using [text frequency - inverse document frequency](https://www.tidytextmining.com/tfidf.html) (TF-IDF) to fill in the values. TF-IDF is used because it supresses words that appear commonly across all documents through the inverse document frequency term. This eliminates the need to specify stopwords to remove from the corpus, which is tedious and might not be too accurate as some stopwords also depend on the unique characteristics of each corpus. We will also lemmatize the words using the [textstem](https://rdrr.io/cran/textstem/) package to better capture the meaning of each document by converting various forms of words to their root word (e.g. 'running', 'ran', and 'run' will be reduced to 'run'). Another approach is stemming, where t   he ends of words are truncated based on heuristics to remove derivational and inflectional affixes (e.g. -s, -ed, -es, -ing, -tion). Past research ([di Nanzio, Vezzani, 2018](http://ceur-ws.org/Vol-2253/paper45.pdf)) has shown that lemmatization and stemming are comparable in terms of performance. We will go with lemmatization for this project because the output are base words, which are more understandable.

```{r}
dtm <- article_df[,c('doc_id','text')] %>%
  unnest_tokens(word,text) 

#lemmatize words
dtm$word <- lemmatize_words(dtm$word)

dtm <- dtm %>%
  count(doc_id, word, sort=TRUE)

dtm <- dtm %>%
  bind_tf_idf(word,doc_id, n)



```
### 3.2.2. Answer


## 3.3. QUESTION 3 
_Given the data sources provided, use visual analytics to identify potential official and unofficial relationships among GASTech, POK, the APA, and Government. Include both personal relationships and shared goals and objectives. Provide evidence for these relationships. Please limit your answer to 6 images and 400 words._

### 3.3.1. Solution Approach and Data Wrangling

### 3.3.2. Answer

# References



